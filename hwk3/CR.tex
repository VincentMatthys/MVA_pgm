\documentclass[12pt,a4paper,onecolumn]{article}
\input{packages}
\input{macros}

\usetikzlibrary{positioning}

% ------------------------ General informations --------------------------------
\title{Math M2 Probabilistic graphical models 2017/2018}
\author{Vincent Matthys}
\graphicspath{{images/}}
% http://www.cedar.buffalo.edu/~srihari/CSE574/Chap13/Ch13.2-HiddenMarkovModels.pdf
% https://www.cs.cmu.edu/~epxing/Class/10701-08s/recitation/em-hmm.pdf
% ------------------------------------------------------------------------------



\begin{document}
\begin{tabularx}{0.8\textwidth}{@{} l X r @{} }
	{\textsc{Master MVA}}                   &  & \textsc{Homework 3} \\
	\textsc{Probabilistic graphical models} &  & {Vincent Matthys}   \\
\end{tabularx}
\vspace{1.5cm}
\begin{center}
	\rule[11pt]{5cm}{0.5pt}

	\textbf{\LARGE \textsc{Compte-rendu du devoir 3}}
	\vspace{0.5cm}\\
	Vincent Matthys\\
	\rule{5cm}{0.5pt}
	\vspace{1.5cm}
\end{center}

\section{HMM implementation}



\setcounter{subsection}{-1}
\subsection{Notations}

On désignera par \(M\) le nombre d'états cachés (4 dans notre cas).
On notera \(\bm{u} = \{u_t\}_{t \in [\![1;T]\!]}\), \(\bm{q} = \{q_t\}_{t \in [\![1;T]\!]}\), \(\bm{\theta} = \left(\pi, A, \bm{\mu}, \bm{\Sigma}\right)\) où \(\bm{\mu} = \{\mu_k\}_{k \in [\![1;M]\!]}\) et \(\bm{\Sigma} = \{\Sigma_k\}_{k \in [\![1;M]\!]}\).
On utilisera également le \textit{one-hot encoding} sur \(\bm{q}\), de sorte que \(q_t^i\), variable binaire, représente la i\up{e} composante de \(q_t\).

\begin{figure}[H]
	\centering
	\begin{tikzpicture}[x=1.6cm,y=1.1cm]
		\begin{scope}[auto, every node/.style={draw,circle,minimum size=3em,inner sep=1},node distance=2cm]
			% Latent nodes
			\node[latent](q1){$q_1$};
			\node[latent, right=of q1](q2){$q_2$};
			\node[latent, right=of q2](dots1){$\dots$};
			\node[latent, right=of dots1](qt){$q_t$};
			\node[latent, right=of qt](qt1){$q_{t+1}$};
			\node[latent, right=of qt1](dots2){$\dots$};
			\node[latent, right=of dots2](qT){$q_T$};

			\edge[]{q1}{q2};
			\edge[]{q2}{dots1};
			\edge[]{dots1}{qt};
			\edge[]{qt}{qt1};
			\edge[]{qt1}{dots2};
			\edge[]{dots2}{qT};

			% observed nodes
			\node[obs, below=of q1](u1){$u_1$};
			\node[obs, below=of q2](u2){$u_2$};
			\node[obs, below=of qt, minimum size=0.8cm](ut){$u_t$};
			\node[obs, below=of qt1, minimum size=0.8cm](ut1){$u_{t+1}$};
			\node[obs, below=of qT](uT){$u_T$};

			\edge[]{q1}{u1}
			\edge[]{q2}{u2}
			\edge[]{qt}{ut}
			\edge[]{qt1}{ut1}
			\edge[]{qT}{uT}
		\end{scope}
	\end{tikzpicture}

	\caption{Modèle graphique représentant le HMM considéré}
	\label{fig_hmm_graph}
\end{figure}

\subsection{}

Le problème de l'inférence dans la chaine de Markov modélisée, à savoir le calcul de \(p(\bm{q} \mid \bm{u})\) est appréhendable par le calcul de la marginale \(p(q_t \mid \bm{u})\) suivant :

\begin{equation}
	\begin{split}
		p(q_t \mid \bm{u}) &= \frac{p(\bm{u} \mid q_t)p(q_t)}{p(\bm{u})}\\
		&= \frac{p(u_1,\dots,u_t \mid q_t)p(u_{t+1},\dots,u_T \mid q_t) p(q_t)}{p(\bm{u})} \quad u_1,\dots,u_t \indep u_{t+1},\dots,u_T \mid q_t\\
		&= \frac{p(u_1,\dots,u_t, q_t)p(u_{t+1},\dots,u_T \mid q_t)}{p(\bm{u})}\\
		&= \frac{\alpha_t(q_t)\beta_t(q_t)}{p(\bm{u})}\\
		&= \frac{\alpha_t(q_t)\beta_t(q_t)}{\sum_{q_t}\alpha_t(q_t)\beta_t(q_t)}\\
		&= \frac{\alpha_t(q_t)\beta_t(q_t)}{\sum_{q_T}\alpha_T(q_T)} \quad \text{puisque} \quad p(\bm{u}) = \sum_{q_T}p(q_T, \bm{u}) = \sum_{q_T}\alpha_T(q_T)\\
	\end{split}
	\label{filtering_eq}
\end{equation}

avec \(\alpha_t(q_t) = p(u_1,\dots,u_t, q_t)\) et \(\beta_t(q_t) = p(u_{t+1},\dots,u_T \mid q_t)\)

On peut alors aussi exprimer \(p(q_t, q_{t+1} \mid \bm{u})\) en fonction de \(\alpha_t\) et \(\beta_t\) :

\begin{equation}
	\begin{split}
		p(q_t, q_{t+1} \mid \bm{u}) &= \frac{p(\bm{u} \mid q_t, q_{t+1}) p (q_t, q_{t+1})}{p(\bm{u})}\\
		&= \frac{p(\bm{u} \mid q_t, q_{t+1}) p (q_{t+1} \mid q_t) p (q_t)}{p(\bm{u})}\\
		&= \frac{p(u_1, \dots, u_t \mid q_t, q_{t+1}) p(u_{t+1}, \dots, u_T \mid q_t, q_{t+1}) p(q_{t+1} \mid q_t) p (q_t)}{p(\bm{u})}\\
		&= \frac{p(u_1, \dots, u_t \mid q_t) p(u_{t+1}, \dots, u_T \mid q_{t+1}) p(q_{t+1} \mid q_t) p (q_t)}{p(\bm{u})}\\
		&= \frac{p(u_1, \dots, u_t \mid q_t) p(u_{t+1} \mid q_{t+1}) p(u_{t+2}, \dots, u_T \mid q_{t+1}) p(q_{t+1} \mid q_t) p (q_t)}{p(\bm{u})}\\
		&= \frac{p(u_1, \dots, u_t, q_t) p(u_{t+1} \mid q_{t+1}) p(u_{t+2}, \dots, u_T \mid q_{t+1}) p(q_{t+1} \mid q_t)}{p(\bm{u})}\\
		&= \frac{\alpha_t(q_t) p(u_{t+1} \mid q_{t+1}) \beta_t(q_{t+1}) A_{q_t, q_{t+1}}}{p(\bm{u})}\\
	\end{split}
\end{equation}
où \(A_{q_t, q_{t+1}} = p(q_{t+1} \mid q_t)\) et \(p(u_{t+1} \mid q_{t+1})\) suit une loi gaussienne.

À des fins d'implémentation, on considèrera les logarithmes des \(\alpha\) et \(\beta\), \(\tilde{\alpha}\) et \(\tilde{\beta}\) de sorte que les formules de récursion peuvent se réecrire :
\begin{equation}
	\left\{
	\begin{split}
		\ln\alpha_t(q_t) &= \ln p(u_t \mid q_t) + \ln\sum_{q_{t-1}}\left(e^{\ln \alpha_{t-1}(q_{t-1})}A_{q_{t-1}, q_{t}}\right)\\
		\ln\beta_t(q_t) &= \ln\sum_{q_{t+1}}\left(e^{\ln \beta_{t+1}(q_{t+1})}A_{q_{t}, q_{t+1}}p(y_{t+1}\mid q_{t+1})\right)\\
	\end{split}
	\right.
\end{equation}

\begin{equation}
	\left\{
	\begin{split}
		\tilde{\alpha}_t(q_t) &= \ln p(u_t \mid q_t) + \ln\sum_{q_{t-1}}\left(e^{\tilde{\alpha}_{t-1}(q_{t-1})}A_{q_{t-1}, q_{t}}\right)\\
		\tilde{\beta}_t(q_t) &= \ln\sum_{q_{t+1}}\left(e^{\tilde{\beta}_{t+1}(q_{t+1})}A_{q_{t}, q_{t+1}}p(y_{t+1}\mid q_{t+1})\right)\\
	\end{split}
	\right.
\end{equation}

et que la tâche d'inférence dite de \textit{smoothing} en équation~\eqref{filtering_eq} s'écrit :

\begin{equation}
	\begin{split}
		\ln p(q_t \mid \bm{u}) &= \ln\alpha_t(q_t) + \ln\beta_t(q_t) - \ln \sum_{q_T}\alpha_T(q_T)\\
		\ln p(q_t \mid \bm{u}) &= \tilde{\alpha}_t(q_t) + \tilde{\beta}_t(q_t) - \ln \sum_{q_T}e^{\tilde{\alpha}_T(q_T)}\\
	\end{split}
\end{equation}

La tâche d'inférence dite de \textit{filtering} est également ajoutée en normalisant \(\alpha_t\) :
\begin{equation*}
	\alpha_t^{\text{norm}}(q_t) = p(q_t \mid u_1, \dots, u_t) = \frac{\alpha_t(q_t)}{\sum_{q_t}\alpha_t(q_t)}
\end{equation*}
et est disponible par le calcul des \(\alpha^{\text{norm}}\) dans le code fourni.

\subsection{}

\begin{figure}[H]
	\centering
	\includegraphics[width = 1.0\textwidth]{2_smoothing.eps}
	\caption{Smoothing result for the first 100 datapoints}
	\label{fig_2_smoothing}
\end{figure}
La figure donnant le résulat du smoothing des 100 premier points est présentée en figure~\ref{fig_2_smoothing}. On y constate notamment une alternance importante entre les états 1 et 2, qui représentent également la majorité des états les plus probables.


\subsection{}

On exprime la log-vraisemblance incomplète de la manière suivante :
\begin{equation}
	\begin{split}
		\ell(\bm{\theta} ; \bm{u}) &= \ln \sum_{\bm{q}}p\left(\bm{q}, \bm{u} \mid \bm{\theta}\right)\\
		&= \ln \sum_{\bm{q}}r(\bm{q} \mid \bm{u}) \frac{p\left(\bm{q}, \bm{u} \mid \bm{\theta}\right)}{r(\bm{q} \mid \bm{u})} \quad \text{pour r quelconque}\\
		&\ge \sum_{\bm{q}}r(\bm{q} \mid \bm{u}) \ln \frac{p\left(\bm{q}, \bm{u} \mid \bm{\theta}\right)}{r(\bm{q} \mid \bm{u})} \quad \text{par l'inégalité de Jensen}\\
		&\ge \mathbb{E}_{r}[\ell_c(\bm{\theta} ; \bm{q}, \bm{u})]- H(r(\bm{q} \mid \bm{u})) = \mathcal{L}(r, \bm{\theta}, \bm{u})\\
	\end{split}
\end{equation}


avec \(\ell_c(\bm{\theta} ; \bm{q}, \bm{u})\), log-vraisemblance complète, dont l'expression est la suivante :
\begin{equation}
	\begin{split}
		\ell_c(\bm{\theta} ; \bm{q}, \bm{u}) &= \ln p\left(\bm{q}, \bm{u} \mid \bm{\theta}\right) \\
		&= \sum_{t=1}^{T}\ln p(u_t \mid q_t, \bm{\mu}, \bm{\Sigma}) + \sum_{t=2}^T \sum_{i = 1}^{M} \sum_{j = 1}^{M} q_{t-1}^i q_t^j \ln A_{ij} + \sum_{i = 1}^M q_1^i \ln \pi_i\\
		&= 	\sum_{t=1}^{T}\sum_{k=1}^M q_t^k \left(-\ln2\pi -\frac{1}{2}\ln|\Sigma_k| - \frac{1}{2}(u_t - \mu_k)^{\intercal}\Sigma_k^{-1}(u_t - \mu_k)\right)\\
		&+ \sum_{t=2}^T \sum_{i = 1}^{M} \sum_{j = 1}^{M} q_{t-1}^i q_t^j \ln A_{ij} + \sum_{i = 1}^M q_1^i \ln \pi_i\\
	\end{split}
	\label{eq_llh_c}
\end{equation}

On a donc une borne inférieure de la log-vraisemblance incomplète, \(\mathcal{L}(r, \bm{\theta}, \bm{u})\). L'algorithme EM consiste en une montée alternée de cette borne inférieure par rapport à \(r\) et \(\bm{\theta}\). En choisissant \(r(\bm{q} \mid \bm{u}) = p(\bm{q} \mid \bm{u}, \bm{\theta})\), on maximise celle-ci, de sorte que les deux étapes de l'algorithme se résument de la manière suivante :
\begin{equation}
	\begin{split}
		E-step \quad r^{(s+1)} &= \operatorname{arg}\max_{r} \mathcal{L}(r, \bm{\theta}^{(s)}, \bm{u}) = p(\bm{q} \mid \bm{u}, \bm{\theta}^{(s)})\\
		M-step \quad \bm{\theta}^{(s+1)} &= \operatorname{arg}\max_{\bm{\theta}} \mathcal{L}(r^{(s+1)}, \bm{\theta}, \bm{u}) = \operatorname{arg}\max_{\bm{\theta}} \mathbb{E}_{r^{(s+1)}}(\ell_c(\bm{\theta} ; \bm{q}, \bm{u}))\\
	\end{split}
	\label{eq_EM_steps}
\end{equation}
où \(s\) dénote le nombre d'itérations de l'algorithme.

D'après l'équation~\eqref{eq_llh_c}, on observe que \(m_{ij} \coloneqq \sum_{t=2}^T q_{t-1}^i q_t^j\) est la statistique exhaustive pour \(A_{ij}\), que \(q_1^i\) est la statistique exhaustive pour \(\pi_i\). Les estimateurs de maximum de log-vraisemblance complète pour \(A\) et \(\pi\) sont donc :

\begin{align}
	\widehat{\pi}_{i} & = q_1^i                              \label{eq_pi_MLE}  \\
	\widehat{A}_{ij}  & = \frac{m_{ij}}{\sum_{k = 1}^M m_{ik}} \label{eq_A_MLE}
\end{align}
D'autre part, grâce aux calculs déjà effectués pour les estimateurs de vraisemblance de la mixture de gaussiennes, on obtient les estimateurs de log-vraisemblance complète des paramètres d'émission :
\begin{align}
	\widehat{\mu_k}    & = \frac{\sum_{t = 1}^T q_{t}^k u_t}{\sum_{t = 1}^T q_{t}^k}                                                     \\
	\widehat{\Sigma_k} & = \frac{\sum_{t = 1}^T q_t^k\left(u_t - \mu_k\right)\left(u_t - \mu_k\right)^{\intercal}}{\sum_{t = 1}^T q_t^k}
\end{align}

Ces estimateurs doivent être pris sous l'espérance précise obtenue lors de l'étape E de l'algorithme EM, donnée par l'équation~\eqref{eq_EM_steps}, afin que la maximisation de la log-vraisemblance complète induise une augmentation de la log-vraisemblance incomplète lors de l'itération \(s + 1\).

En reprenant l'estimateur de \(\pi\) obtenu en équation~\eqref{eq_pi_MLE}, on obtient l'estimateur à l'itération \((s + 1)\) :
\begin{equation}
	\begin{split}
		\widehat{\pi_{i}}^{(s + 1)} &= \mathbb{E}_{\bm{q} \mid \bm{u}, \bm{\theta}^{(s)}}[q_1^i] \\
		&= \mathbb{E}_{q_1 \mid \bm{u}, \bm{\theta}^{(s)}}[q_1^i] \\
		&= p(q_1^i = 1 \mid \bm{u}, \bm{\theta}^{(s)}) \coloneqq \gamma_1^{i, (s+1)}
	\end{split}
\end{equation}

En reprenant l'estimateur de \(A_{ij}\) obtenue en équation~\eqref{eq_A_MLE}, il faut déterminer \(\mathbb{E}_{\bm{q} \mid \bm{u}, \bm{\theta}^{(s)}}[m_{ij}]\). On a :
\begin{equation*}
	\begin{split}
		\mathbb{E}_{\bm{q} \mid \bm{u}, \bm{\theta}^{(s)}}[m_{ij}] &= \sum_{t=2}^T \mathbb{E}_{\bm{q} \mid \bm{u}, \bm{\theta}^{(s)}}\left[q_{t-1}^i q_t^j\right]\\
		&= \sum_{t=2}^T p\left(q_{t-1}^i = 1, q_t^j = 1 \mid \bm{u}, \bm{\theta}^{(s)}\right)\\
		&\coloneqq \sum_{t=2}^T \xi^{ij, (s+1)}_{t-1, t}
	\end{split}
\end{equation*}
D'où l'estimateur à l'itération \((s+1)\) pour \(A\) :
\begin{equation}
	\widehat{A}_{ij}^{(s+1)} = \frac{\sum_{t=2}^T \xi^{ij, (s+1)}_{t-1, t}}{\sum_{k = 1}^M\sum_{t=2}^T \xi^{ik, (s+1)}_{t-1, t}} = \frac{\sum_{t=2}^T \xi^{ij, (s+1)}_{t-1, t}}{\sum_{t=2}^T \gamma^{i, (s+1)}_{t-1}}
\end{equation}

De la même façon, on obtient les estimateurs à l'itération \((s+1)\) pour les \(\mu_k\) et \(\Sigma_k\) :

\begin{equation}
	\begin{aligned}
		\widehat{\mu_k}^{(s+1)}    & = \frac{\sum_{t = 1}^T \gamma_{t}^{k,(s+1)} u_t}{\sum_{t = 1}^T \gamma_{t}^{k,(s+1)}} \quad \forall k \in [\![1;M]\!]                                                                     \\
		\widehat{\Sigma_k}^{(s+1)} & = \frac{\sum_{t = 1}^T \gamma_{t}^{k,(s+1)}\left(u_t - \mu_k^{(s)}\right)\left(u_t - \mu_k^{(s)}\right)^{\intercal}}{\sum_{t = 1}^T \gamma_{t}^{k,(s+1)}} \quad \forall k \in [\![1;M]\!] \\
	\end{aligned}
\end{equation}

L'implémentation du calcul des \(\gamma\) et \(\xi\) est succintement détaillée et fait intervenir le calcul des \(\alpha^{\text{norm}}\) du problème de \textit{filtering}.
\begin{equation}
	\begin{split}
		\gamma_t(q_t) &= p(q_t \mid u_1, \dots, u_T)\\
		&= \sum_{q_{t+1}}p(q_t, q_{t+1} \mid u_1, \dots, u_T)\\
		&= \sum_{q_{t+1}} \frac{\alpha_t(q_t)A_{q_t, q_{t+1}}}{\sum_{q_t}\alpha_t(q_t)A_{q_t, q_{t+1}}}\gamma_{t+1}(q_{t+1})\\
		&= \sum_{q_{t+1}} \frac{{\alpha_t^{\text{norm}}}(q_t)A_{q_t, q_{t+1}}}{\sum_{q_t}{\alpha_t^{\text{norm}}}(q_t)A_{q_t, q_{t+1}}}\gamma_{t+1}(q_{t+1})\\
	\end{split}
\end{equation}

\begin{equation}
	\begin{split}
		\xi_{t,t+1}(q_t, q_{t+1}) &= p(q_t, q_{t+1} \mid \bm{u})\\
		&= \frac{\alpha_t(q_t)p(u_{t+1} \mid q_{t-1})\gamma_{t+1}(q_{t+1})A_{q_t, q_{t+1}}}{\alpha_{t+1}(q_{t+1})}\\
		&= \frac{\alpha^{\text{norm}}_t(q_t)p(u_{t+1} \mid q_{t-1})\gamma_{t+1}(q_{t+1})A_{q_t, q_{t+1}}}{\alpha^{\text{norm}}_t(q_{t+1})} \times \frac{\sum_{q_t}\alpha_t(q_t)}{\sum_{q_{t+1}}\alpha_{t+1}(q_{t+1})}\\
	\end{split}
\end{equation}

Au niveau de l'implémentation, on notera qu'on utilise les \(\alpha^{\text{norm}}\) qui ne changent aucunement la formule pour \(\gamma\). Pour \(\xi\), il faut prendre en compte la différence de normalisation entre les \(\alpha^{\text{norm}}\).


\subsection{}

La partie d'implémentation est effectuée toujours en python dans la classe \textit{HMM}.

\subsection{}

\begin{figure}[H]
	\centering
	\includegraphics[width = 1.0\textwidth]{5_em_llh}
	\caption{Evolution de la log-vraisemblance en fonction du nombre d'itérations de EM pour le modèle de HMM}
	\label{fig_5_llh_em}
\end{figure}

La log-vraisemblance en apprentissage et en test est représentée en figure~\ref{fig_5_llh_em}. On constate qu'il suffit de 2-3 itérations de l'algorithme EM pour converger vers de nouveaux paramètres, en partant de ceux déterminés par mixture de gaussienne. On constate également que la vraisemblance en apprentissage est toujours supérieure à la vraisemblance de test, ce qui constitue un résultat attendu.

\subsection{}

Les valeurs de log-vraisemblance des différents modèles rencontrés sont présentées en table~\ref{tab_6_llh}. On constate que le modèle avec la meilleure log-vraisemblance est le modèle chaine de Markov, suivi du modèle de mixture de gaussiennes générales. C'est un résultat attendu, puisque l'odre correspond avec le degré de complexité du modèle : plus celui-ci est complexe (c'est-à-dire avec de degré de liberté), plus il peut coller aux données. On constate également que la log-vraisemblance sur les données de test est inférieure à celle obtenue sur les données d'entrainement, à l'exception du modèle de mixture gaussiennes isotropiques pour lequel la différence observée est minime entre les données d'entrainement et de test. On notera cette vraisemblance de GM isotropic en test est la seule vraisemblance différente de celle notée dans la solution du devoir 2 fourni par M. Obozinski, dans laquelle la vraissemblance en question est de \(-2.6841~10^3\) et est bien  inférieure à celle obtenue pour le même modèle sur les données d'entrainement.

\begin{table}
	\centering
	\begin{tabular}{*{3}{c}}
		\toprule
		Modèle       & Donées d'entraînement & Données de test  \\
		\midrule
		isotropic GM & -2.6396               & -2.6146          \\
		general GM   & -2.3277               & -2.4092          \\
		HMM          & \(\bm{-1.8968}\)      & \(\bm{-1.9555}\) \\
		\bottomrule
	\end{tabular}
	\caption{Valeur \(\times10^3\) de la log-vraiemblance des données d'apprentissage et des données de test pour différents modèles. GM signifie Gaussian Mixture (isotropic pour des covariances diagonales, general pour des covariances quelconques), HMM signifie Hidden Markov Model}
	\label{tab_6_llh}
\end{table}

\subsection{}

L'algorithme de décodage de Viterbi permet de déterminer la séquence d'états la plus probable  \(\bm{q} = q_1,\dots,q_T\) étant donnée une séquence d'observation \(\bm{u} = u_1,\dots,u_t\) pour des paramètres de modèle donnés \(\bm{\theta}\). Au lieu de calculer la log-vraisemblance d'une séquence d'états étant donnée les observations, ce qui reviendrait à faire une \(\alpha\) récursion pour chaque séquence d'états, on utilise l'algorithme \(max-product\) sur le graphe du modèle de Markov en figure~\ref{fig_hmm_graph}, en prenant comme racine de l'arbre le noeud \(q_1\). Ainsi, on obtient la relation de récurence suivante :

\begin{align}
	\delta_{t\rightarrow t+1}(q_{t+1})      & = \max_{q_t}\left(\delta_{(t-1)\rightarrow t}(q_t) A_{q_t,q_{t+1}} \right) p(u_{t+1} \mid q_{t+1})                    \\
	\log \delta_{t\rightarrow t+1}(q_{t+1}) & = \max_{q_t}\left(\log \delta_{(t-1)\rightarrow t}(q_t) + \log A_{q_t,q_{t+1}} \right) + \log p(u_{t+1} \mid q_{t+1})
\end{align}

Une fois estimée ce message pour chaque point, on considère la succession d'états qui a permis d'obtenir ce message, notée \(\bm{q}^* = \operatorname{arg}\max_{\bm{q}} p(\bm{q} \mid \bm{u})\), que l'on obtient en partant du noeud \(q_T\) :

\begin{align}
	q_T^* & = \operatorname{arg}\max_{q_T} \delta_{(T-1)\rightarrow T}(q_T)                                                      \\
	q_t^* & = \operatorname{arg}\max_{q_t} \delta_{(t-1)\rightarrow t}(q_t) A_{q_t, q_{t+1}^*} \quad \forall t \in [\![1;T-1]\!]
\end{align}

D'où le pseudo-code suivant :

\begin{algorithm}[H]
	\SetAlgoLined
	\KwResult{$\bm{q^*}$}
	$\delta_1(j) = \pi_j p(u_1
		\mid q_1 = j)$\;
	\For{$i=2$ \thinspace to T}{
		$\delta_t(j) = \max_i \left(\delta_{t-1}(i) + \log A_{i,j}\right) + \log p(u_t \mid q_t = j)$\;
	}
	$q_T^* = \operatorname{arg}\max_{j} \delta_T(j)$\;
	\For{$i=T-1$ \thinspace to 1}{
		$q_t^* = \operatorname{arg}\max_{j} \delta_{t}(j) A_{j, q_{t+1}^*}$\;
	}
	\caption{Viterbi decoding}
\end{algorithm}


\subsection{}

Le résultat du décodage de Viterbi sur les 100 premiers points des données d'entrainement avec les paramètres du modèle HMM obtenus par EM sur ces mêmes données est présenté en figure~\ref{fig_8_viterbi}. On a également superposé les centres des clusters ainsi que l'ellipsiode correspondant à \(90~\%\) de la masse de chaque gaussienne associée. On constate que l'attribution des points à chaque cluster est cohérent avec les résultats précédemment obtenus lors de la modélisation par mixture de gaussiennes générales.

\begin{figure}[H]
	\centering
	\includegraphics[width = 1.0\textwidth]{8_viterbi_decoding}
	\caption{Visualisation de la séquence d'états la plus probable sur les 100 premiers points du jeu d'entrainement obtenu par décodage de Viterbi}
	\label{fig_8_viterbi}
\end{figure}

\subsection{}

Le calcul des probabilités marginales de chaque état \(q_t\) étant donnée la séquence complète des observations \(\bm{u}\) a été décrite en équation~\eqref{filtering_eq} lors de l'\(\alpha-\beta\) récursion. On utilise les paramètres \(\alpha\) et \(\beta\) obtenus après convergence de EM sur les données d'entrainement, et on représente les marginales de chacun des 4 états des 100 premiers points du jeu de test en figure~\ref{fig_9_filtering}. On peut y observer une alternance presque exclusive des états 2 et 3.

\begin{figure}[H]
	\centering
	\includegraphics[width = 1.0\textwidth]{9_filtering}
	\caption{Probabilités marginales des états sur le jeu de test pour les 100 premiers points}
	\label{fig_9_filtering}
\end{figure}

\subsection{}

L'état le plus probable pour chacun de ces mêmes points est représenté en figure~\ref{fig_10_most_likely}. On y constate clairement l'alternance entres les états 2 et 3, entrecoupés de périodes de 2 à 9 points en état 4.

\begin{figure}[H]
	\centering
	\includegraphics[width = 1.0\textwidth]{10_most_likely}
	\caption{Etat le plus probable pour chacun des 100 premiers points du jeu de test}
	\label{fig_10_most_likely}
\end{figure}

\subsection{}

\begin{figure}[H]
	\centering
	\begin{subfigure}[b]{0.85\textwidth}
		\centering
		\includegraphics[width = 1.0\textwidth]{11_viterbi_decoding_test}
		\caption{Décodage selon Viterbi}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.85\textwidth}
		\centering
		\includegraphics[width = 1.0\textwidth]{10_marginals}
		\caption{Décodage selon les probabilités marginales}
	\end{subfigure}
	\caption{Décodage des 100 premiers points du jeu de test suivant Viterbi ou les probabilités marginales}
	\label{fig_11_decodage_test}
\end{figure}

Les décodages obtenus par Vierbi et par les probabilités marginales pour les 100 premiers points du jeu de test sont présentés en figure~\ref{fig_11_decodage_test}. On remarque que les décoadges par les deux méthodes sont identiques.

\subsection{}

Si le nombre d'états n'était pas connu, il faudrait faire une sélection de modèle en comparant la log-vraisemblance obtenue par les différents modèles HMM (à différents nombre d'états) et conserver le modèle donnant la meilleure vraisemblance en test.

\end{document}
