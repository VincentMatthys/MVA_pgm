\documentclass[12pt,a4paper,onecolumn]{article}
\input{packages}
\input{macros}

% ------------------------ General informations --------------------------------
\title{Math M2 Probabilistic graphical models 2017/2018}
\author{Vincent Matthys}
\graphicspath{{images/}}
% ------------------------------------------------------------------------------

\begin{document}
\begin{tabularx}{0.8\textwidth}{@{} l X r @{} }
	{\textsc{Master MVA}}                   &  & \textsc{Homework 2} \\
	\textsc{Probabilistic graphical models} &  & {Vincent Matthys}   \\
\end{tabularx}
\vspace{1.5cm}
\begin{center}
	\rule[11pt]{5cm}{0.5pt}

	\textbf{\LARGE \textsc{Compte-rendu du devoir 2}}
	\vspace{0.5cm}\\
	Vincent Matthys\\
	\rule{5cm}{0.5pt}
	\vspace{1.5cm}
\end{center}

\section{Indépendance conditionnelle et factorisations}

\subsection{}

\begin{equation}
	\begin{split}
		X \indep Y \mid Z &\Leftrightarrow p(x, y \mid z) = p(x \mid z) p(y \mid z) \qquad \forall x, y, z \quad  \text{t.q.} \quad p(z) > 0 \\
		&\Leftrightarrow p(x, y, z) = p(x \mid z) p(y \mid z) p(z)\qquad \forall x, y, z \quad  \text{t.q.} \quad p(z) > 0 \\
		&\Leftrightarrow \frac{p(x, y ,z)}{p(y , z)} = p(x \mid z) \frac{p(y \mid z) p(z)}{p(y, z)} \qquad \forall x, y, z \quad  \text{t.q.} \quad p(y, z) > 0 \\
		&\Leftrightarrow p(x \mid y, z) = p(x \mid z) \qquad \forall x, y, z \quad  \text{t.q.} \quad p(y, z) > 0 \\
	\end{split}
\end{equation}

\subsection{}

Etant donné le modèle graphique orienté \(G\) :

\begin{equation}
	\begin{split}
		p \in \mathcal{L}(G) &\Leftrightarrow \forall x, y, z, t \quad p(x, y, z, t) = p(x)p(y)p(z \mid  x, y) p(t \mid z)
	\end{split}
\end{equation}



\subsection{}


\section{Distributions factorisant sur un graphe}

\subsection{}


\subsection{}


\clearpage

\section{Entropie et information mutuelle}

\subsection{Entropie}

\subsubsection{}

Avec les conventions définies :

\begin{equation}
	\begin{split}
		p_X(x) &= \mathbb{P}(X = x) < 1 \Rightarrow p_X(x)\log(p_X(x)) < 0 \Rightarrow -\sum_{x \in \mathcal{X}} p_X(x)\log(p_X(x)) = H(X) \geq 0 \\
		H(X) &= 0 \Rightarrow \forall x \in \mathcal{X},\, p_X(x)\log(p_X(x)) = 0 \Rightarrow \forall x \in \mathcal{X},\, p_X(x) = 1 \Rightarrow \text{X is constant with probability 1}
	\end{split}
	\label{eq_31a}
\end{equation}

\subsubsection{}

Par définition de la Kullback-Leibler divergence il vient :

\begin{equation}
	\begin{split}
		D(p_X \parallel q) &= \sum_{x \in \mathcal{X}}p_X(x)\log\frac{p_X(x)}{q(x)}\\
		&= -\sum_{x \in \mathcal{X}}p_X(x)\log q(x) - H(X) \\
		&= -\sum_{x \in \mathcal{X}}p_X(x)\log\frac{1}{|X|} - H(X) \quad \text{puisque} \quad \forall x \in \mathcal{X},\, q(x) = \frac{1}{k} = \frac{1}{k}\\
		&= \log k\sum_{x \in \mathcal{X}}p_X(x) - H(X)\\
		&= \log k - H(X) \quad \text{car} \quad \sum_{x \in \mathcal{X}}p_X(x) = 1
	\end{split}
	\label{eq_31b}
\end{equation}

\subsubsection{}
Avec les équations~\eqref{eq_31a} et \eqref{eq_31b}, on a directement

\begin{equation}
	\log k - H(X) = D(p_X \parallel q) \leq \log k
\end{equation}

\subsection{Information mutuelle}

\subsubsection{}
Par définition de l'information mutuelle :

\begin{equation}
	\begin{split}
		I(X_1, X_2) &= \sum_{(x_1,x_2)\in\mathcal{X}_1\times\mathcal{X}_2} p_{1, 2}(x_1,x_2) \log \frac{p_{1, 2}(x_1,x_2)}{p_1(x_1)\,p_2(x_2)}\\
		I(X_1, X_2) &= D(p_{1, 2} \parallel p_1p_2) \quad \text{par définition de la Kullback-Leibler divergence}
	\end{split}
\end{equation}

Or la Kullback-Leibler divergence est positive pour toute paire \((p_{1, 2}, p_1p_2)\) de distributions, donc \(I(X_1, X_2) \geq 0\).

\subsubsection{}

Toujours avec la défintion de l'information mutuelle :

\begin{equation}
	\begin{split}
		I(X_1, X_2) &= \sum_{(x_1,x_2)\in\mathcal{X}_1\times\mathcal{X}_2} p_{1, 2}(x_1,x_2) \log \frac{p_{1, 2}(x_1,x_2)}{p_1(x_1)\,p_2(x_2)}\\
		I(X_1, X_2) &= \sum_{(x_1,x_2)\in\mathcal{X}_1\times\mathcal{X}_2} p_{1, 2}(x_1,x_2) \log p_{1, 2}(x_1,x_2)\\ &- \sum_{(x_1,x_2)\in\mathcal{X}_1\times\mathcal{X}_2} p_{1, 2}(x_1,x_2) \log \left(p_1(x_1)p_2(x_2)\right)\\
		I(X_1, X_2) &= -H(X_1, X_2)\\
		&- \sum_{x_1 \in\mathcal{X}_1} \left(\sum_{x_2 \in\mathcal{X}_2}p_{1, 2}(x_1,x_2)\right) \log \left(p_1(x_1)\right)\\
		&- \sum_{x_2 \in\mathcal{X}_2} \left(\sum_{x_1 \in\mathcal{X}_1}p_{1, 2}(x_1,x_2)\right) \log \left(p_2(x_2)\right)\\
		I(X_1, X_2) &= -H(X_1, X_2)\\
		&- \sum_{x_1 \in\mathcal{X}_1} p_{1}(x_1) \log \left(p_1(x_1)\right)\\
		&- \sum_{x_2 \in\mathcal{X}_2} p_{2}(x_2) \log \left(p_2(x_2)\right)\\
		I(X_1, X_2) &= H(X_1) + H(X_2) - H(X_1, X_2)
	\end{split}
	\label{eq_32b}
\end{equation}

Et ainsi l'information mutuelle peut s'écrire uniquement à partir des entropies de \(X_1\), \(X_2\) et de \((X_1, X_2)\).

\subsubsection{}

D'après l'éqiation~\eqref{eq_32b}, on peut réecrire :

\begin{equation}
	\begin{split}
		H(X_1, X_2) - \left(H(X_1) + H(X_2)\right) &=  - I(X_1, X_2)\\
		H(X_1, X_2) - \left(H(X_1) + H(X_2)\right) &\leq 0 \quad \text{puisque},\, I(X_1, X_2) \geq 0
	\end{split}
	\label{eq_32c}
\end{equation}

D'après l'équation~\eqref{eq_32c}, pour \(p_1\) et \(p_2\) données, l'entropie maximale correspond à \(I(X_1, X_2) = 0\), c'est-à-dire \(D(p_{1, 2} \parallel p_1p_2) = 0\) d'après l'équation~\eqref{eq_31b}. Or la Kullback-Leibler divergence s'anulle si et seulement si les distributions sont identiques. On a alors \(p_{1,2} = p_1p_2\), ce qui correspond à \(X_1 \indep X_2\).

\section{Gaussian Mixtures}

\subsection{K-means}

\begin{figure}[H]
	\centering
	\includegraphics[width = \textwidth]{4_a}
	\caption{Visualisation du clustering obtenu par k-means des données d'entrainement sur 9 initialisations aléatoires des centroïdes}
	\label{fig_4_K_means}
\end{figure}

En figure~\ref{fig_4_K_means} sont présentés les résultats du clustering des données d'entraînement \textit{EMGaussian.data} avec 9 initialisations aléatoires distinctes des centroïdes. La condition d'arrêt est remplie quand la variation de distorsion est inférieure à 1. On a utilisé l'angle des centroïdes finaux afin d'artificiellement repérer par les mêmes couleurs et mêmes chiffres les clusters situés dans des zones similaires du plan. On constate que la distorsion de ces 9 initialisations différentes ne diffère que par le 3\up{e} chiffre significatif, avec une moyenne de \(3.239~10^{3}\) et avec un écart-type \(1.335\). Les minima locaux de cette distorsion ont donc des valeurs très semblables sur ces itérations, et les positions des centroïdes finaux sont regroupées dans la table~\ref{tab_4_a}. On peut constater que les variations autour des positions moyennes sont de l'ordre du dixième dans le pire des cas, correspondant à une variation en distance de l'ordre de \(0.3~\%\). Néanmoins on constate que les points à la frontière des clusters ont tendance à basculer, tantôt vers l'un ou l'autre des clusters, notamment à la frontière des clusters jaune bleu et rouge, où les points proches de \((0,0)\) sont facilement attribué par l'humain au cluster 1, rouge, qui présente une forme très allongée, différente des autres formes de clusters. Or la clusterisation par k-means ne permet pas de prendre en compte cette spécificité de forme. En définitif, les clusters trouvés par k-means sont, pour ce jeu de données d'entraînement, très stables pour la majorité des initialisations aléatoires et permette de séparer en première approximation, indépendamment de la forme du cluster, les points suivant un consensus proche de celui de l'homme, à l'exception de quelques points centraux.

\begin{table}
	\centering
	\begin{tabular}{r|r|r}
		\hline
		Centroïde & \(\mu_x \pm \sigma_x\) & \(\mu_y \pm \sigma_y\) \\\hline
		1         & \(-3.747\pm 0.068\)    & \(-4.193\pm 0.081\)    \\\hline
		2         & \( -2.177 \pm 0.044\)  & \( 4.060\pm 0.078 \)   \\\hline
		3         & \( 3.794\pm 0.007\)    & \( 5.038\pm 0.048\)    \\\hline
		4         & \( 3.483\pm 0.107\)    & \( -2.795\pm 0.105 \)  \\\hline
	\end{tabular}
	\caption{Positions des centroïdes sur les 9 initialisations aléatoires}
	\label{tab_4_a}
\end{table}

\subsection{Mixture de gaussiennes isotropiques}

Dans le cas d'une mixture de gaussiennes isotropiques, on peut réecrire l'étape de maximisation suivant :

\begin{equation}
	\begin{split}
		\bm{\theta}^{(t + 1)} &= \operatorname{arg}\max_{\bm{\theta}}\left(\mathbb{E}_{\bm{z} \mid \bm{x}, \bm{\theta}^{(t)}}\right)\\
		\bm{\theta}^{(t + 1)} &= \operatorname{arg}\max_{\bm{\theta}}\left(\sum_{n = 1}^N\sum_{k = 1}^4q_{nk}^{(t)}\left(\log\pi_k -
		\frac{1}{2}\log|\Sigma_k|-\frac{1}{2}\left(x_i - \mu_k\right)^{\intercal}\Sigma_k^{-1}\left(x_i - \mu_k\right)\right)\right)
	\end{split}
\end{equation}

où \(\bm{\theta} = \left(\bm{\pi}, \bm{\mu}, \bm{\Sigma}\right)\) et \(q_{nk}^{(t)} = \mathbb{P}\left(z_n = k | x_n, \bm{\theta}^{(t)}\right)\) avec \(z_n\) variable latente associée à \(x_n\).

On peut alors séparer la maximisation suivant \(\bm{\pi}\) et suivant \(\left(\bm{\mu}, \bm{\Sigma})\right)\), de sorte que :

\begin{equation}
	\begin{split}
		\bm{\pi}^{(t + 1)} &= \operatorname{arg}\max_{\bm{\pi}}\left(\sum_{n = 1}^N\sum_{k = 1}^4q_{nk}^{(t)}\log\pi_k\right) \quad \text{avec}\quad\forall n,\,\sum_{k = 1}^4q_{nk} = 1\\
		(\bm{\mu}^{(t + 1)}, \bm{\Sigma}^{(t + 1)}) &= \operatorname{arg}\max_{\bm{\mu}, \bm{\Sigma}}\left(\sum_{n = 1}^N\sum_{k = 1}^4q_{nk}\left(
		-\frac{1}{2}\log|\Sigma_k|-\frac{1}{2}\left(x_i - \mu_k\right)^{\intercal}\Sigma_k^{-1}\left(x_i - \mu_k\right)\right)\right)\\
	\end{split}
	\label{eq_isotrop}
\end{equation}

Les deux problèmes de minimisation en~\eqref{eq_isotrop} sont différentiables et concaves (le deuxième étant analogue au calcul du MLE pour une distribution normale) et on a :

\begin{equation}
	\begin{split}
		\frac{\partial\left(\sum_{n = 1}^N\sum_{k = 1}^4q_{nk}^{(t)}\log\pi_k^{(t+1)}\right)}{\partial\pi_k^{(t+1)}} &= \sum_{n = 1}^Nq_{nk}^{(t)} - \lambda \pi_k^{(t+1)} = 0 \quad \text{avec} \quad \lambda \in \mathbb{R}\\
		\pi_k^{(t+1)} &\propto \sum_{n = 1}^Nq_{nk}^{(t)}\\
		\pi_k^{(t+1)} &= \frac{\sum_{n = 1}^Nq_{nk}^{(t)}}{\sum_{k = 1}^4\sum_{n = 1}^Nq_{nk}^{(t)}} = \frac{1}{N}\sum_{n = 1}^Nq_{nk}^{(t)}
	\end{split}
\end{equation}


\begin{equation}
	\begin{split}
		\frac{\partial\left(\sum_{n = 1}^N\sum_{k = 1}^4q_{nk}^{(t)}\left(
		-\frac{1}{2}\left(x_i - \mu_k\right)^{\intercal}\Sigma_k^{-1}\left(x_i - \mu_k\right)\right)\right)}{\partial\mu_k} &=
		\sum_{n = 1}^Nq_{nk}^{(t)}\left(\Sigma_k^{-1}(x_i-\mu_k^{(t+1)})\right) = 0\\
		\mu_k^{(t + 1)} &= \frac{\sum_{n = 1}^Nq_{nk}^{(t)}x_i}{\sum_{n = 1}^Nq_{nk}^{(t)}}
	\end{split}
\end{equation}

Avec l'hypothèse \(\bm{\Sigma_k} = \sigma_k^2\bm{I}\) d'isotropie, il vient :

\begin{equation}
	\begin{split}
		\frac{\partial\left(\sum_{n = 1}^N\sum_{k = 1}^4q_{nk}^{(t)}\left(
		-\frac{1}{2}\log|\Sigma_k|-\frac{1}{2}\left(x_i - \mu_k\right)^{\intercal}\Sigma_k^{-1}\left(x_i - \mu_k\right)\right)\right)}{\partial\sigma_k^2} &=
		\sum_{n = 1}^Nq_{nk}\left(\frac{1}{\sigma_k^2} -\frac{1}{\sigma_k^4}\left(x_i-\mu_k\right)^{\intercal}\left(x_i-\mu_k\right)\right)\\
		\sigma_k^2 &= \frac{\sum_{n = 1}^Nq_{nk}\left(x_i-\mu_k\right)^{\intercal}\left(x_i-\mu_k\right)}{\sum_{n = 1}^Nq_{nk}}
	\end{split}
\end{equation}

\subsection{Mixture de gaussiennes générales}

\subsection{subsection name}


\end{document}
