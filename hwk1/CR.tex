\documentclass[12pt,a4paper,onecolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{amscd}
\usepackage{amsthm}
\usepackage{physics}
\usepackage[left=2.2cm,right=2.2cm,top=2cm,bottom=2cm]{geometry}
\usepackage{textcomp,gensymb} %pour le °C, et textcomp pour éviter les warning
\usepackage{graphicx} %pour les images
\usepackage{caption}
\usepackage{subcaption}
\usepackage[colorlinks=true,
	breaklinks=true,
	citecolor=blue,
	linkcolor=blue,
	urlcolor=blue]{hyperref} % pour insérer des liens
\usepackage{epstopdf} %converting to PDF
\usepackage[export]{adjustbox} %for large figures

\usepackage{array}
\usepackage{dsfont}% indicatrice : \mathds{1}

% -------------------------- Mathematics ---------------------------------------
\usepackage{mathrsfs, amsmath, amsfonts, amssymb}
\usepackage{bm}
\newcommand{\R}{\mathbb{R}} % For Real space
% ------------------------------------------------------------------------------


% -------------------------- Code format ---------------------------------------
\usepackage[numbered,framed]{matlab-prettifier}
\lstset{
	style              = Matlab-editor,
	basicstyle         = \mlttfamily,
	escapechar         = '',
	mlshowsectionrules = true,
}
% ------------------------------------------------------------------------------

% ------------------------- Blbiographie --------------------------------------
% \usepackage[backend=biber, style=science]{biblatex}
% \addbibresource{biblio.bib}
% ------------------------------------------------------------------------------

% ------------------------- Color table ----------------------------------------
\usepackage{multirow}
% \usepackage[table]{xcolor}
% \definecolor{maroon}{cmyk}{0,0.87,0.68,0.32}/
% ------------------------------------------------------------------------------

\setcounter{tocdepth}{4} %Count paragraph
\setcounter{secnumdepth}{4} %Count paragraph
\usepackage{float}

\usepackage{graphicx} % for graphicspath
% \graphicspath{{../images/}}

\usepackage{array,tabularx}
% \newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
% \newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
% \newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}


% ------------------------------ TITLE -----------------------------------------
\title{Math M2 Probabilistic graphical models 2017/2018}
\author{Vincent Matthys}
% ------------------------------------------------------------------------------

% ------------------------------ NUMEROTATION ----------------------------------
% \renewcommand{\thesubsection}{\alph{subsection}}
% ------------------------------------------------------------------------------


\begin{document}
\begin{tabularx}{0.8\textwidth}{@{} l X r @{} }
	{\textsc{Master MVA}}                   &  & \textsc{Homework 1} \\
	\textsc{Probabilistic graphical models} &  & {Vincent Matthys}   \\
\end{tabularx}
\vspace{1.5cm}
\begin{center}
	\rule[11pt]{5cm}{0.5pt}

	\textbf{\LARGE \textsc{Compte-rendu du devoir 1}}
	\vspace{0.5cm}\\
	Vincent Matthys\\
	\rule{5cm}{0.5pt}
	\vspace{1.5cm}
\end{center}

\section{Leaning in discrete graphical models}

Etant donné que \(z\) et \(x\) sont des variables à valeurs discrètes prenant respectivement \(M\) et \(K\) valeurs, on peut procéder au \textit{one hot encoding}, notant respectivement \(\bm{Z}\) et \(\bm{X}\) leur encodage sur, respectivement \(\R^M\) et \(\R^K\). Ainsi, on peut écrire :

\[
	p(z = m) = P(Z_m = 1) = \pi_m
\]
\[
	p(x = k | z = m) = p(X_k = 1 | Z_m = 1) = \theta_{mk}
\]
En supposant que l'on ait un échantillon de \(n\) observations de \((x, z)\), on peut exprimer la probabilité jointe d'une obersvation \(i\) :

\begin{equation}
	\begin{split}
		p(x^{(i)}, z^{(i)} ; \bm{\pi}, \bm{\theta}) & = p(z^{(i)} ; \bm{\pi}) p(x^{(i)} | z^{(i)} ; \bm{\theta})                                          \\
		p(x^{(i)}, z^{(i)} ; \bm{\pi}, \bm{\theta}) & = \prod_{m=1}^M \prod_{k=1}^K \theta_{mk}^{X_k^{(i)} Z_m^{(i)}} \prod_{l = 1}^M\pi_l^{Z_l^{(i)}}
	\end{split}
	\label{1_joint}
\end{equation}

On peut alors écrire la log-vraisemblance de l'échantillon dans le modèle \(( \bm{\pi}, \bm{\theta})\), composé d'observations i.i.d., en utilisant~\eqref{1_joint}:

\begin{equation}
	\begin{split}
		\ell( \bm{\pi}, \bm{\theta}) & = \sum_{i = 1}^n \ln(p(x^{(i)}, z^{(i)} ; \bm{\pi}, \bm{\theta}))                                                                        \\
		& = \sum_{i = 1}^n\left(\ln\left(\prod_{m=1}^M \prod_{k=1}^K \theta_{mk}^{X_k^{(i)} Z_m^{(i)}} \prod_{l = 1}^M\pi_l^{Z_l^{(i)}}\right)\right)                   \\
		& = \sum_{i = 1}^n\left(\sum_{m=1}^M \sum_{k=1}^K \ln(\theta_{mk}^{X_k^{(i)} Z_m^{(i)}}) + \sum_{l = 1}^M\ln(\pi_l^{Z_l^{(i)}})\right)                          \\
		& = \sum_{i = 1}^n\left(\sum_{m=1}^M \sum_{k=1}^K X_k^{(i)} Z_m^{(i)}\ln(\theta_{mk}) + \sum_{l = 1}^M {Z_l^{(i)}}\ln(\pi_l)\right)                             \\
		& = \sum_{m=1}^M \sum_{k=1}^K \left(\sum_{i = 1}^n X_k^{(i)} Z_m^{(i)}\right)\ln(\theta_{mk}) + \sum_{l = 1}^M\left(\sum_{i = 1}^n {Z_l^{(i)}}\right)\ln(\pi_l) \\
		& = \sum_{m=1}^M \sum_{k=1}^K \alpha_{mk}\ln(\theta_{mk}) + \sum_{l = 1}^M \beta_l\ln(\pi_l)
	\end{split}
	\label{1_logl}
\end{equation}

avec
\begin{align*}
	\alpha_{mk} & = \sum_{i = 1}^n X_k^{(i)} Z_m^{(i)} \\
	\beta_l     & = \sum_{i = 1}^n {Z_l^{(i)}}
\end{align*}

D'après~\eqref{1_logl}, la log-vraisemblance est donc strictement concave en chaque composantes de \(\bm{\pi}\) et \(\bm{\theta}\), par combinaison linéaire de logarithmes. D'autre part, on a les contraintes linéaires suivantes :
\begin{equation}
	\begin{split}
		\sum_{m = 1}^M \pi_m - 1 &= 0\\
		\sum_{k = 1}^K \theta_{mk} - 1 &= 0 \,,  \forall m : 1..M
	\end{split}
\end{equation}

On peut donc écrire le Lagrangien, en utilisant les multiplicateurs de Lagrange correspondants, associé au problème de maximisation de la log-vraisemblance~\eqref{1_logl} :

\begin{equation}
	\mathcal{L}(\bm{\pi}, \bm{\theta}, \lambda, \bm{\gamma}) = \sum_{m=1}^M \sum_{k=1}^K \alpha_{mk}\ln(\theta_{mk}) + \sum_{l = 1}^M \beta_l\ln(\pi_l) - \lambda\left(\sum_{m = 1}^K \pi_m - 1\right) - \bm{\gamma}^\intercal \begin{pmatrix}
		\vdots                         \\
		\sum_{k = 1}^K \theta_{mk} - 1 \\
		\vdots
	\end{pmatrix}
	\label{1_lagrang}
\end{equation}

Le problème admet une solution unique, notée \( (\bm{\widehat{\pi_{MLE}}}, \bm{\widehat{\theta_{MLE}}}) \) que l'on trouve par dérivation de~\eqref{1_lagrang}

\subsection{Par rapport à \protect\(\pi_m\)}

\[
	\frac{\partial \mathcal{L}(\bm{\pi}, \bm{\theta}, \lambda, \bm{\gamma})}{\partial \pi_m} = \frac{\beta_m}{\pi_m} - \lambda = 0
	\implies \pi_m \propto \beta_m
	\implies \pi_m = \frac{\beta_m}{\sum_{m=1}^M \beta_m}= \frac{\sum_{i = 1}^n {Z_m^{(i)}}}{\sum_{m=1}^M \sum_{i = 1}^n {Z_m^{(i)}}}
\]
D'où finalement :
\begin{equation}
	\left(\widehat{\pi_{MLE}}\right)_m = \frac{1}{n} \sum_{i = 1}^n {Z_m^{(i)}}
	\label{1_pi}
\end{equation}

\subsection{Par rapport à \protect\(\theta_{mk}\)}

\[
	\frac{\partial \mathcal{L}(\bm{\pi}, \bm{\theta}, \lambda, \bm{\gamma})}{\partial \theta_{mk}} = \frac{\alpha_{mk}}{\theta_{mk}} - \gamma_m = 0 \implies \theta_{mk} \propto \alpha_{mk}
	\implies \theta_{mk} = \frac{\alpha_{mk}}{\sum_{k = 1}^K \alpha_{mk}} = \frac{\sum_{i = 1}^n X_k^{(i)} Z_m^{(i)}}{\sum_{i = 1}^n \left(\sum_{k=1}^K X_k^{(i)}\right) Z_m^{(i)}}
\]

D'où finalement :

\begin{equation}
	\left( \widehat{{\theta_{MLE}}} \right)_{mk} = \frac{\sum_{i = 1}^n X_k^{(i)} Z_m^{(i)}}{\sum_{i = 1}^n Z_m^{(i)}}
\end{equation}

On pourra remarquer que l'on peut s'affranchir de l'hypothèse implicite qu'aucune composante de \(\bm{\pi}\) ni de \(\bm{\theta}\) n'est nulle. En effet, si tel est le cas, alors la probabilité d'observer une telle observation est nulle.

\section{Linear classification}

\subsection{Generative model : Linear Discriminant Analysis}


\end{document}
